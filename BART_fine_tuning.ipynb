{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MNUG0JGS0NU"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/CSE_497/Final"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing dependencies"
      ],
      "metadata": {
        "id": "zzRiRFRTALZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "!pip install bert_score"
      ],
      "metadata": {
        "id": "ot0391wATV0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BartForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, BartTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, AutoTokenizer, AutoConfig\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import evaluate\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "-uwajzJzTbHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset"
      ],
      "metadata": {
        "id": "f4npYlLuDGg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"final_raw_data.csv\")\n",
        "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "eval_dataset = Dataset.from_pandas(eval_df)\n",
        "dataset = DatasetDict({\"train\": train_dataset, \"eval\": eval_dataset})"
      ],
      "metadata": {
        "id": "zSljC-WWTdKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "gradLiYeqBMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuring the LLM"
      ],
      "metadata": {
        "id": "4nZsycC7DIsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Load fine-tuned model and tokenizer for futher training or evaluation\n",
        "# model_name = \"./results_flan_long/checkpoint-7371\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "jox0kaxoTdIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving the model to GPU for faster training"
      ],
      "metadata": {
        "id": "bsey0zZzAk3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "DCbnp0QaTdFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the dataset before training-"
      ],
      "metadata": {
        "id": "AwwmGGYxAsyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(examples):\n",
        "    inputs = tokenizer(examples['problem'], max_length=512, truncation=True, padding=\"max_length\")\n",
        "    outputs = tokenizer(examples['approach'], max_length=1024, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    inputs['labels'] = outputs['input_ids']\n",
        "    inputs['labels'] = [\n",
        "        [(label if label != tokenizer.pad_token_id else -100) for label in labels]\n",
        "        for labels in inputs['labels']\n",
        "    ]\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "x3Ss1ajxTdCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = dataset.map(preprocess, batched=True)"
      ],
      "metadata": {
        "id": "MTcXl41gxMhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Evaluation metrics"
      ],
      "metadata": {
        "id": "QBM5qUCQDOyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the BERTScore metric\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Compute BERTScore (using default settings)\n",
        "    result = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
        "\n",
        "    # BERTScore returns precision, recall, and F1 for each example\n",
        "    # We can average over the F1 scores\n",
        "    avg_f1 = np.mean(result[\"f1\"])\n",
        "\n",
        "    # Return a dictionary with the metric\n",
        "    return {\"bertscore_f1\": round(avg_f1, 4)}\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------\n",
        "\n",
        "# # Load the ROUGE metric\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value * 100 for key, value in result.items()}\n",
        "\n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# # Load the BLEU metric\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = bleu.compute(predictions=decoded_preds,\n",
        "                            references=decoded_labels)\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(\n",
        "        pred != tokenizer.pad_token_id) for pred in preds]\n",
        "\n",
        "    result = {'bleu': result}\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "DSdLgQwkMKKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "BQGMyjz3MMHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results_bart_long\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    metric_for_best_model=\"rouge1\",\n",
        "    logging_steps=100,\n",
        "    push_to_hub=False,\n",
        "    no_cuda=False,\n",
        "    eval_accumulation_steps = 2,\n",
        "    predict_with_generate=True\n",
        ")"
      ],
      "metadata": {
        "id": "ftOW80bHTiJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"eval\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "ZhDGxrYGTpH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Begin training and save the final model explicitly"
      ],
      "metadata": {
        "id": "o_Nd45IzCr5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"./fine_tuned_bart_final\")"
      ],
      "metadata": {
        "id": "Fw7xswk7p28F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manually evaluating the model"
      ],
      "metadata": {
        "id": "SKSuto8fC9Qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_approach(problem):\n",
        "    input_text = f\"{problem}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, min_length=1024, max_length=1024)\n",
        "    # print(outputs)\n",
        "    approach = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return approach\n",
        "\n",
        "\n",
        "problem = \"Provide an approach for detecting spam emails.\"\n",
        "generated_approach = generate_approach(problem)\n",
        "print(generated_approach)"
      ],
      "metadata": {
        "id": "GFDk96NsTpEc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}